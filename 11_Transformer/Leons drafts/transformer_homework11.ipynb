{"cells":[{"cell_type":"markdown","metadata":{"id":"R6NkoFcQv-DJ"},"source":["# Transformer\n","\n","## Table of Contents\n","\n"]},{"cell_type":"markdown","source":["## Installs"],"metadata":{"id":"sFio4Db-wFBp"}},{"cell_type":"code","source":["!pip install -q -U tensorflow-text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXGDcbs0wDLX","executionInfo":{"status":"ok","timestamp":1676780183874,"user_tz":-480,"elapsed":9487,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}},"outputId":"126edba8-be1c-4bcd-a91a-5c48f64d50f7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cj4D5-pxwWjs","executionInfo":{"status":"ok","timestamp":1676780206604,"user_tz":-480,"elapsed":11392,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}},"outputId":"1aed044e-e724-435e-d451-da49d62a4f2b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"markdown","metadata":{"id":"MYYnfh-Pv-DK"},"source":["## Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"szgVeXkEv-DL","executionInfo":{"status":"ok","timestamp":1676780216434,"user_tz":-480,"elapsed":6212,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"outputs":[],"source":["import tensorflow_text as tf_txt\n","import tensorflow as tf\n","import sentencepiece as sp"]},{"cell_type":"markdown","metadata":{"id":"_rTgYkRzv-DM"},"source":["## Text Processing"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Msjv_8jXyr2v","executionInfo":{"status":"ok","timestamp":1676780244016,"user_tz":-480,"elapsed":21237,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}},"outputId":"8a3c6c9f-eca8-4ca4-dd3b-fe35a8edb256"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sEk7DxUtv-DN","executionInfo":{"status":"ok","timestamp":1676780249968,"user_tz":-480,"elapsed":1072,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"outputs":[],"source":["bible = open('/content/drive/My Drive/Uni/7/IANNwTF/bible.txt', 'r').read()"]},{"cell_type":"code","source":["len(bible)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wi9bEcuM1qvD","executionInfo":{"status":"ok","timestamp":1676780296648,"user_tz":-480,"elapsed":290,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}},"outputId":"5620f4d6-91ac-499a-b306-21997b4dff0e"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4332496"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLHZ_cEJv-DN"},"outputs":[],"source":["def normalizetext(text):\n","  notwanted = '. ; , : ? ! -'\n","  notwanted = notwanted.split()\n","  \n","  text = text.replace('\\n', ' ')\n","  for w in notwanted:\n","    text = text.replace(w, ' ')\n","  \n","  text = text.lower()\n","  return text"]},{"cell_type":"markdown","metadata":{"id":"bqzwh8viv-DO"},"source":["## SentencePiece Tokenizer"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"XBpAR8C1v-DP","executionInfo":{"status":"ok","timestamp":1676780311356,"user_tz":-480,"elapsed":408,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"outputs":[],"source":["sp.SentencePieceTrainer.train(\n","    input='/content/drive/My Drive/Uni/7/IANNwTF/bible.txt', model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=5000)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"6Sft8f_xv-DP","executionInfo":{"status":"ok","timestamp":1676780315301,"user_tz":-480,"elapsed":278,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"outputs":[],"source":["# deserialize the trained model file to load it in the correct format\n","trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n","\n","# load the model as a tokenizer that can be used inside a tensorflow model\n","# cant do it yet because I need to get the tensorflow-text to work\n","tokenizer = tf_txt.SentencepieceTokenizer(\n","    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n","    add_bos=False, add_eos=False, return_nbest=False, name=None\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"OKL8QKUqv-DQ","executionInfo":{"status":"ok","timestamp":1676780324050,"user_tz":-480,"elapsed":4238,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"outputs":[],"source":["tokens = tokenizer.tokenize(bible)"]},{"cell_type":"markdown","metadata":{"id":"QCruMFCkv-DQ"},"source":["## Sliding window"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"dRMW-Sr7v-DR","executionInfo":{"status":"ok","timestamp":1676780329529,"user_tz":-480,"elapsed":1283,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"outputs":[],"source":["SEQ_LENGTH = 128\n","windows = tf_txt.sliding_window(data=tokens, width=SEQ_LENGTH+1, axis=0) # still have to import tensorflow_text"]},{"cell_type":"code","source":["windows.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DsMMlDjo4ISY","executionInfo":{"status":"ok","timestamp":1676780332141,"user_tz":-480,"elapsed":312,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}},"outputId":"cc0a3d5b-ca48-4c6d-8d14-cc401234cb51"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([1026906, 129])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["input = windows[:, :SEQ_LENGTH]\n","input_ds = tf.data.Dataset.from_tensor_slices(input)"],"metadata":{"id":"7VrfD_oE2Rh2","executionInfo":{"status":"ok","timestamp":1676780333936,"user_tz":-480,"elapsed":469,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["target = windows[:, 1:]\n","target_ds = tf.data.Dataset.from_tensor_slices(target)"],"metadata":{"id":"s29ejd5x3gIM","executionInfo":{"status":"ok","timestamp":1676780339070,"user_tz":-480,"elapsed":721,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.zip((input_ds, target_ds))"],"metadata":{"id":"5bb4DSPi5U06","executionInfo":{"status":"ok","timestamp":1676780344798,"user_tz":-480,"elapsed":270,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"8iB57NM84_UJ","executionInfo":{"status":"ok","timestamp":1676780346977,"user_tz":-480,"elapsed":6,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["for batch in dataset.take(1):\n","  print(batch[1][0].shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpnVqbNaPHFV","executionInfo":{"status":"ok","timestamp":1676780349375,"user_tz":-480,"elapsed":838,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}},"outputId":"0f13443a-5526-4f24-96f1-45750c918003"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["128\n"]}]},{"cell_type":"markdown","source":["## The Components\n","\n","### EmbeddingLayer"],"metadata":{"id":"FeNJ9_IqKf6e"}},{"cell_type":"code","source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","  \n","  def __init__(self, embed_dim, vocab_size):\n","    super(PositionalEmbedding, self).__init__()\n","\n","    super().__init__()\n","    self.embed_dim = embed_dim\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True) \n","    self.pos_encoding = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)\n","\n","  \n","  def call(self, input, training=False): # input.shape = (batch_size, sequence_length)\n","    lookup = tf.range(0, input[0].shape[0], 1)\n","\n","    embedding = self.embedding(input)\n","    pos_encoding = self.pos_encoding(lookup) # is this really correct?\n","\n","    return embedding + pos_encoding"],"metadata":{"id":"eT4Uc8ftI7yr","executionInfo":{"status":"ok","timestamp":1676786227797,"user_tz":-480,"elapsed":278,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["### Transformer Block"],"metadata":{"id":"i0BE0Xhp5h83"}},{"cell_type":"code","source":["class TransformerBlock(tf.keras.layers.Layer):\n","\n","    def __init__(self, num_heads, embed_dim, dff, dropout_rate):\n","        super(TransformerBlock, self).__init__()\n","        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n","        self.dense2 = tf.keras.layers.Dense(embed_dim)\n","        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n","        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n","        self.layernorm1 = tf.keras.layers.LayerNormalization()\n","        self.layernorm2 = tf.keras.layers.LayerNormalization()\n","        self.add1 = tf.keras.layers.Add()\n","        self.add2 = tf.keras.layers.Add()\n","        ## still have to add the dense layers and dropout layers\n","\n","    def call(self, x):\n","        attn_output = self.mha(\n","            query=x,\n","            value=x,\n","            key=x,\n","            use_causal_mask = True)\n","        attn_output = self.dropout1(attn_output)\n","        x = self.add1([x, attn_output])\n","        ln_out = self.layernorm1(x)\n","        x = self.dense1(ln_out)\n","        x = self.dense2(x)\n","        x = self.dropout2(x)\n","        x = self.add2([ln_out, x])\n","        out = self.layernorm2(x)\n","\n","        return out"],"metadata":{"id":"F4G5VJNs6dkh","executionInfo":{"status":"ok","timestamp":1676786346617,"user_tz":-480,"elapsed":351,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["## The Transformer"],"metadata":{"id":"UtGacvzF-Pf8"}},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","  \n","    def __init__(self, sentence_piece_tokenizer, embed_dim, vocab_size, num_heads, dff, dropout_rate):\n","        super(Transformer, self).__init__()\n","\n","        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        self.metrics_list = [\n","                            tf.keras.metrics.SparseCategoricalCrossentropy(name=\"loss\"),\n","                            tf.keras.metrics.Accuracy(name=\"acc\")\n","                           ]\n","\n","        self.tokenizer = sentence_piece_tokenizer\n","        self.pos_embed = PositionalEmbedding(embed_dim=embed_dim, vocab_size=vocab_size)\n","        self.transformer_block = TransformerBlock(num_heads=num_heads, embed_dim=embed_dim, dff=dff, dropout_rate=dropout_rate)\n","        self.dense = tf.keras.layers.Dense(units=vocab_size)\n","\n","    #2. call method (forward computation)\n","    def call(self, img, training=False):\n","        x = self.pos_embed(img)\n","        x = self.transformer_block(img)\n","        x = self.dense(x)\n","        return x\n","\n","    #3. metrics property\n","    @property\n","    def metrics(self):\n","        # return a list with all metrics in the model\n","        return self.metrics_list\n","\n","\n","    #4 reset all metrics object\n","    def reset_metrics(self):\n","        for metric in self.metrics:\n","            metric.reset_states()\n","\n","    #5 training step method\n","    def train_step(self, data):\n","        # update the state of the metrics according to loss\n","        # return a dictionary with metrics name as keys an metric results\n","        \n","        img, label = data\n","        with tf.GradientTape() as tape:\n","            output = self(img, training=True)\n","            loss = self.loss(label, output)\n","        \n","    \n","        gradients = tape.gradient(loss, self.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","\n","        #update the state of the metrics according to loss\n","        #self.metrics[0].update_state(label, output)\n","        for metric in self.metrics:\n","            metric.update_state(label, output)\n","\n","        # return a dictionary with metric names as keys and metric results as values\n","        return {m.name : m.result() for m in self.metrics}\n","    \n","\n","    #6. test step method\n","    def test_step(self, data):\n","        img, label = data\n","        output = self(img, training=False)\n","        loss = self.loss(label, output)\n","        #self.metrics[0].update_state(label, output)\n","        for metric in self.metrics:\n","            metric.update_state(label, output)\n","\n","        return {\"val_\"+m.name : m.result() for m in self.metrics}\n","\n","\n","    def generate_text(self, prompt, output_length, top_k):\n","        tokens = self.sentence_piece_tokenizer.tokenize(prompt)\n","        tokens = tf.expand_dim(tokens, axis=0)\n","        logits = self(tokens)\n"],"metadata":{"id":"OMNkZMk--I-q","executionInfo":{"status":"ok","timestamp":1676786342488,"user_tz":-480,"elapsed":3,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"BALwGtPgO7Pt"}},{"cell_type":"code","source":["import tqdm\n","def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer, save_path):\n","  #1. iterate over epochs\n","  for e in range(epochs):\n","    #2. train steps on all batchs in the training data\n","    for data in tqdm.tqdm(train_ds):\n","      ret,metrics = model.train_step(data)\n","    # 3. log and print data metrics\n","    with train_summary_writer.as_default():\n","      for metric in model.metrics:\n","        print(metric)\n","        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=e)\n","\n","    # print the metrics\n","    print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n","\n","\n","    #4 reset the metrics\n","    model.reset_metrics()\n","\n","    #5. evaluate on validation data\n","    for data in val_ds:\n","      metrics = model.test_step(data)\n","    \n","    #6. log validation metrics\n","    with val_summary_writer.as_default():\n","      for metric in model.metrics:\n","        print(metric)\n","        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=e)\n","\n","    # print the metrics\n","    print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n","    \n","    #7. reset metric objects\n","    model.reset_metrics()\n","\n","  #8 save model weights\n","  model.save_weights(save_path)"],"metadata":{"id":"ftQq2HZAN3WK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing"],"metadata":{"id":"x3AUKAp3O9b_"}},{"cell_type":"code","source":["num_heads = 4\n","sentence_piece_tokenizer = tokenizer\n","embed_dim = 128\n","vocab_size = 5000\n","dff = 128\n","dropout_rate = 0.1\n","model = Transformer(num_heads=num_heads, \n","                    sentence_piece_tokenizer=sentence_piece_tokenizer,\n","                    embed_dim=embed_dim,\n","                    vocab_size=vocab_size,\n","                    dff=dff,\n","                    dropout_rate=dropout_rate)"],"metadata":{"id":"MXfkKR75O-Yc","executionInfo":{"status":"ok","timestamp":1676786352662,"user_tz":-480,"elapsed":391,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"H6SvnEriPvoL","executionInfo":{"status":"error","timestamp":1676786465325,"user_tz":-480,"elapsed":3059,"user":{"displayName":"Leon Ackermann","userId":"07588838393303295680"}},"outputId":"b8857c0a-c80d-40ce-fd71-60c05428b89b"},"execution_count":33,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-0a3d47183ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QRwLfGlYQumk"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"iannwtf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a3449bbb043929c6f13b514689ff91c66257e0787e2d8bb0eba8270d3f40eacf"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}