{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "t4G87fB76l-T"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
        "print(categories[:10])\n",
        "category = 'candle'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdvGFDYr88wO",
        "outputId": "8e50a7c1-1105-4e0e-9fca-49b84dba67a0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'aircraft carrier', b'airplane', b'alarm clock', b'ambulance', b'angel', b'animal migration', b'ant', b'anvil', b'apple', b'arm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a folder to download the original drawings into.\n",
        "# We chose to use the numpy format : 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]\n",
        "\n",
        "if not os.path.isdir('npy_files'):\n",
        "    os.mkdir('npy_files')\n",
        "\n",
        "#category = \"googleDataset\"\n",
        "\n",
        "url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'  \n",
        "urllib.request.urlretrieve(url, f'npy_files/{category}.npy')\n",
        "\n",
        "images = np.load(f'npy_files/{category}.npy')\n",
        "print(f'{len(images)} images to train on')\n",
        "\n",
        "# You can limit the amount of images you use for training by setting :\n",
        "train_images = images[:10000]\n",
        "# You should also define a samller subset of the images for testing..\n",
        "test_images = images[10000:12000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BINIttsk8-HD",
        "outputId": "45343903-c795-4697-be7e-a6b1d74761f6"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141545 images to train on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4wFqjsk9AD8",
        "outputId": "8303fddd-3a36-4093-db79-c6012ed3226e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(141545, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data):\n",
        "    data = tf.data.Dataset.from_tensor_slices(data.reshape(len(data), 28,28,1))\n",
        "    #data = tf.data.Dataset.from_tensor_slices(data)\n",
        "    \n",
        "    #reshape\n",
        "    #data = data.map(lambda img: tf.reshape(img, (28,28,1)))\n",
        "\n",
        "    data = data.map(lambda img: (tf.cast(img, tf.float32)))\n",
        "\n",
        "    # normalize input to gaussian distribution or divide by 128\n",
        "    data = data.map(lambda img: ((img/128)-1))\n",
        "\n",
        "    # keep the progess in memory\n",
        "    data = data.cache()\n",
        "    #mnist = mnist.shuffle(1000) \n",
        "    data = data.batch(32) # 32 image in one batch\n",
        "    data = data.prefetch(20) # prepare 20 next datapoints \n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "LF8gAxw89Bg0"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = prepare_data(train_images)\n",
        "test_ds = prepare_data(test_images)\n",
        "train_ds.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq6xBj-E-EJm",
        "outputId": "16daa1dd-1137-4adc-bbb8-08213ad64964"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train_ds.take(1):\n",
        "  img = tf.cast(tf.floor(x*128+1), tf.uint32)[-1,:,:,-1]\n",
        "  plt.imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vYV3Z3NyGBAA",
        "outputId": "6b3ea99d-af17-4ddb-92c7-23bf4a9b35c3"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Model**"
      ],
      "metadata": {
        "id": "lhbGGKGGD1Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.layers_list = [\n",
        "      #tf.keras.layers.Input(shape=(32,28,28,1)),\n",
        "      tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', input_shape=(28,28,1)), \n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'), \n",
        "      tf.keras.layers.GlobalMaxPool2D(),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ]\n",
        "\n",
        "  def __call__(self, img, training = False):\n",
        "    for layer in self.layers_list:\n",
        "      img = layer(img, training=training)\n",
        "    return img "
      ],
      "metadata": {
        "id": "ULhEgBTYAJMj"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.transposed_layers_list = [\n",
        "        tf.keras.layers.Dense(7*7*64, input_shape=(100,)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Reshape((7, 7, 64)),\n",
        "        tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', activation='tanh')\n",
        "    ]\n",
        "\n",
        "  def __call__(self, x, training = False):\n",
        "    for layer in self.transposed_layers_list:\n",
        "        x = layer(x, training = training)\n",
        "    return x\n",
        "    \n"
      ],
      "metadata": {
        "id": "MvqIf08RFzof"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(GAN, self).__init__()\n",
        "\n",
        "    self.metrics_list = [\n",
        "                        tf.keras.metrics.Mean(name=\"discriminator_loss\"),\n",
        "                        tf.keras.metrics.Mean(name=\"generator_loss\") \n",
        "                       ]\n",
        "    self.optimizer_disc = tf.keras.optimizers.Adam()\n",
        "    self.optimizer_gen = tf.keras.optimizers.Adam()\n",
        "    self.loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    self.generator = Generator()\n",
        "    self.discriminator = Discriminator()\n",
        "\n",
        "    #2. call method (forward computation)\n",
        "  def call(self, img, noise = tf.random.normal([32,1,100]), training = False):\n",
        "    # create fake image with generator\n",
        "    # store fake image in gen_fake_img\n",
        "    gen_fake_image = self.generator(noise, training = training)\n",
        "    # feed it to dicriminator\n",
        "    # store prediction in fake_img\n",
        "    fake_img = self.discriminator(gen_fake_image, training = training)\n",
        "    # feed real image to discriminator\n",
        "    # store prediction in img\n",
        "    img = self.discriminator(img, training = training) \n",
        "    return img, fake_img, gen_fake_image\n",
        "    \n",
        "\n",
        "  #3. metrics property\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    # return a list with all metrics in the model\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  #4 reset all metrics object\n",
        "  def reset_metrics(self):\n",
        "    for metric in self.metrics:\n",
        "      metric.reset_states()\n",
        "\n",
        "  def train_step(self, data):\n",
        "    noise = tf.random.normal([32,1,100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: #its possible to use just one gradient tape with tf.GradientTape(persistent = True)\n",
        "      # call the GAN model\n",
        "      imgPrediction, fake_imgPrediction, gen_fake_image = self(x, noise, training = True) \n",
        "      # create targets\n",
        "      imgTarget, fake_imgTarget = tf.ones_like(imgPrediction), tf.zeros_like(fake_imgPrediction) #we could do this outside of gradientTape, with tf.ones instead of tf.ones_like\n",
        "\n",
        "      # calculate the discriminator loss\n",
        "      imgLoss = self.loss(imgTarget, imgPrediction)\n",
        "      fake_imgLoss = self.loss(fake_imgTarget, fake_imgPrediction)\n",
        "      discriminator_loss = fake_imgLoss + imgLoss\n",
        "\n",
        "      # calculate the generator loss\n",
        "      generator_loss = self.loss(tf.ones_like(fake_imgPrediction), fake_imgPrediction)\n",
        "\n",
        "    gen_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)\n",
        "    disc_gradients = disc_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)\n",
        "    self.optimizer_gen.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
        "    self.optimizer_disc.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "    # update metrics\n",
        "    self.metrics[0].update_state(discriminator_loss)\n",
        "    self.metrics[1].update_state(generator_loss)\n",
        "    return {m.name : m.result() for m in self.metrics}, gen_fake_image\n",
        "\n",
        "  def test_step(self, data):\n",
        "    noise = tf.random.normal([32,1,100])\n",
        "\n",
        "    # call model and create targets \n",
        "    imgPrediction, fake_imgPrediction, gen_fake_image = self(x, noise, training = False) \n",
        "    imgTarget, fake_imgTarget = tf.ones_like(imgPrediction), tf.zeros_like(fake_imgPrediction) \n",
        "\n",
        "    # calculate the loss\n",
        "    imgLoss = self.loss(imgTarget, imgPrediction)\n",
        "    fake_imgLoss = self.loss(fake_imgTarget, fake_imgPrediction)\n",
        "    discriminator_loss = fake_imgLoss + imgLoss\n",
        "\n",
        "    generator_loss = self.loss(tf.ones_like(fake_imgPrediction), fake_imgPrediction)\n",
        "\n",
        "    # update metrics\n",
        "    self.metrics[0].update_state(discriminator_loss)\n",
        "    self.metrics[1].update_state(generator_loss)\n",
        "\n",
        "    return {\"val_\" + m.name : m.result() for m in self.metrics}, gen_fake_image"
      ],
      "metadata": {
        "id": "NwqmjV0dUBpM"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_test = GAN() \n",
        "\n",
        "model_test(tf.keras.Input((28,28,1)), tf.keras.Input((1,100)))\n",
        "\n",
        "model_test.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WkUqSknbLCQ",
        "outputId": "002f2e7f-ce0c-41d3-b685-34d871aa31e6"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gan_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " generator_14 (Generator)    multiple                  0 (unused)\n",
            "                                                                 \n",
            " discriminator_14 (Discrimin  multiple                 0 (unused)\n",
            " ator)                                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 395,078\n",
            "Trainable params: 388,546\n",
            "Non-trainable params: 6,532\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer, img_summary_writer, save_path): \n",
        "\n",
        "  # 1. iterate over epochs\n",
        "  img = []\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # 2. train step over all batches in training data\n",
        "    for data in tqdm.tqdm(train_ds):\n",
        "      metrics, generated_fake_image = model.train_step(data)\n",
        "\n",
        "\n",
        "    # 3. log and print training metrics \n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.scalar(name = \"discriminator_loss\", data = metrics[\"discriminator_loss\"], step = epoch)\n",
        "      tf.summary.scalar(name = \"generator_loss\", data = metrics[\"generator_loss\"], step = epoch)\n",
        "\n",
        "    print(metrics.items())\n",
        "\n",
        "    # 4. rest metrics\n",
        "    model.reset_metrics()\n",
        "\n",
        "    # 5. evaluate on validation data\n",
        "    for data in val_ds:\n",
        "      metrics, generated_fake_image = model.test_step(data)\n",
        "\n",
        "    img.append(generated_fake_image)\n",
        "    # 6. log validation metric \n",
        "    with val_summary_writer.as_default():\n",
        "      tf.summary.scalar(name = \"val_discriminator_loss\", data = metrics[\"val_discriminator_loss\"], step = epoch)\n",
        "      tf.summary.scalar(name = \"val_generator_loss\", data = metrics[\"val_generator_loss\"], step = epoch)\n",
        "\n",
        "    with img_summary_writer.as_default():\n",
        "      #tensorflow can handle floats in image, we dont have to cast back to int\n",
        "      tf.summary.image(name = \"generated_fake_image\", data = tf.reshape(generated_fake_image, (-1, 28, 28, 1)), step = epoch)\n",
        "\n",
        "    print(metrics.items())\n",
        "\n",
        "    # 7. reset metric objects\n",
        "    model.reset_metrics()\n",
        "\n",
        "  #8. save models weights\n",
        "  model.save_weights(save_path)\n",
        "\n",
        "  # 9. plot generated image for each epoch\n",
        "  no_subplot = 1\n",
        "  for x in img:\n",
        "    x = tf.cast(tf.floor(x*128+1), tf.uint32)[-1,:,:,-1]\n",
        "    plt.subplot(1,10,no_subplot)\n",
        "    plt.imshow(x, cmap = 'gray')\n",
        "    plt.axis('off')\n",
        "    no_subplot += 1"
      ],
      "metadata": {
        "id": "E-1K7pQGrC3P"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_path = f\"logs/train/\"\n",
        "val_log_path = f\"logs/val/\"\n",
        "img_log_path = f\"logs/img/\"\n",
        "\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path + current_time)\n",
        "# log writer for validation metrics\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path + current_time)\n",
        "\n",
        "img_summary_writer = tf.summary.create_file_writer(img_log_path + current_time)\n"
      ],
      "metadata": {
        "id": "0xD-rBynr7Lf"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAN()\n",
        "epochs=10\n",
        "\n",
        "#2. choose a path to save the weights\n",
        "save_path = \"trained_model\"\n",
        "\n",
        "training_loop(model, train_ds, test_ds, 10, train_summary_writer, val_summary_writer, img_summary_writer, save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "egZXdilJshfW",
        "outputId": "c8be1d86-45e6-4fe6-c52f-eafab81f41d7"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:21<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.12199862>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=3.4593759>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=2.1447585>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.1447971>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:21<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.01954305>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=5.287057>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.34065303>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=1.6915325>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:21<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.012027404>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=5.651688>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.11582822>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=3.166764>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:21<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.011502011>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=6.170632>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.033458136>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=6.565122>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:10<00:00,  4.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.0060540903>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=6.443069>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.4577736>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=6.2472444>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:10<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.0049868347>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=6.878602>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.07393301>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=3.5928006>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:21<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.0042057447>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=6.9875417>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=1.0845153>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.5653268>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:21<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.0023877067>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=7.138022>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.07986958>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=9.767504>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:11<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.0014804385>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=7.4488516>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.07181645>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=6.795386>)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [01:10<00:00,  4.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.0011779555>), ('generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=7.7392173>)])\n",
            "dict_items([('val_discriminator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=0.0048444695>), ('val_generator_loss', <tf.Tensor: shape=(), dtype=float32, numpy=7.2492943>)])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9LW14ertsyNG"
      },
      "execution_count": 122,
      "outputs": []
    }
  ]
}